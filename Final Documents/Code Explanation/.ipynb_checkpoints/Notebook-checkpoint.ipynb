{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Analysis & Candidate Ranking System\n",
    "Welcome to the Resume Analysis & Candidate Ranking System Major Project of Batch-4(CSM)! In today's competitive job market, organizations receive a vast number of resumes for each job opening, making the candidate selection process time-consuming and challenging. To address this issue, this system offers an automated solution for analyzing resumes and ranking candidates based on their suitability for a given job description.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [System Architecture](#system-architecture)\n",
    "3. [Zip File Extraction](#zip-file-extraction)\n",
    "4. [Information Extraction](#information-extraction)\n",
    "5. [Analysis of Job Description](#analysis-of-job-description)\n",
    "6. [Analysis of Candidate Resume](#analysis-of-candidate-resume)\n",
    "7. [Applying LDA Model](#applying-lda-model)\n",
    "8. [Calculating Similarity](#calculating-similarity)\n",
    "9. [Ranking Candidates](#ranking-candidates)\n",
    "10. [Clearing Extracted Files](#clearing-extracted-files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## 1. Introduction\n",
    "\n",
    "Imagine a scenario where a public organization, such as a government agency or a non-profit organization, is hiring for a critical role that requires specific skills and qualifications. With limited resources and a high volume of applicants, manual screening of resumes becomes impractical and inefficient. In such cases, an automated resume analysis and candidate ranking system can significantly streamline the hiring process, allowing the organization to identify top candidates quickly and efficiently.\n",
    "\n",
    "This system can be particularly beneficial for public organizations with limited HR resources, enabling them to make data-driven decisions in their hiring process while ensuring fairness and transparency. By leveraging natural language processing techniques and machine learning algorithms, the system evaluates resumes objectively and ranks candidates based on their alignment with the job requirements.\n",
    "\n",
    "Let's explore the components of this system and see how it can revolutionize the candidate selection process for public organizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"system-architecture\"></a>\n",
    "## 2. System Architecture\n",
    "The system architecture consists of several components that work together to analyze resumes and rank candidates:\n",
    "- **Zip File Extraction:** Resumes are extracted from a compressed zip file for further processing.\n",
    "- **Information Extraction:** Data is loaded from both the job description and resumes, and textual content is extracted for analysis.\n",
    "- **Analysis of Job Description:** The job description is analyzed to identify key requirements and skills using TF-IDF.\n",
    "- **Analysis of Candidate Resume:** Each candidate's resume is analyzed to extract relevant features and qualifications using TF-IDF.\n",
    "- **Applying LDA Model:** The LDA (Latent Dirichlet Allocation) model is applied to uncover underlying topics within the job description.\n",
    "- **Calculating Similarity:** Similarity between the job description and each resume is calculated using cosine similarity and LDA topic distributions.\n",
    "- **Ranking Candidates:** Candidates are ranked based on their similarity to the job description.\n",
    "- **Clearing Extracted Files:** Extracted files are cleared to maintain a tidy workspace.\n",
    "\n",
    "Let's delve into each component in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"zip-file-extraction\"></a>\n",
    "## 3. Zip file extraction\n",
    "\n",
    "In this initial step, resumes are extracted from a compressed zip file for further processing. This allows us to access and analyze multiple resumes conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample.pdf', 'sample2.pdf', 'sample3.pdf']\n"
     ]
    }
   ],
   "source": [
    "import zipfile,os\n",
    "\n",
    "zip_path = 'Data/Code.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('Data/Extracted/')\n",
    "print(os.listdir(\"Data/Extracted/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"information-extraction\"></a>\n",
    "## 4. Information Extraction\n",
    "This section focuses on loading data from both the job description and resumes. Resumes are parsed to extract textual content, which is then stored for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data center engineering intern winter 2024 google mumbai maharashtra india intern apprentice minimum currently pursuing bachelor master phd degree computer science related technical field experience installing network linux system ability communicate english fluently apply please complete application 5 2024 winter internships start february 2024 weeks duration internship intended students pursuing final year bachelor master dual degree program computer science related field graduate 2024 start application process need updated cv resume current unofficial official transcript english click apply button page provide required materials appropriate sections pdfs preferred 1 resume section attach updated cv resume please ensure listed anticipated graduation date proficiency statistical software database languages resume along technical experience projects 2 education section attach current recent unofficial official transcript english degree status select attending upload transcript back jobs search preferred familiarity basic concepts network linux systems ability understand develop data center architectures products system design operations maintenance procedures excellent problem solving skills ability use tools produce highly accurate analyses ensure designs satisfy safety requirements available work full time minimum 6 months outside university term time job behind google simple search box one complex technology infrastructures world behind infrastructure diverse group googlers design build keep lights users data center engineering team takes physical design data centers future technical engineering support teams bring ideas life teams use creative approaches reduce operational costs extend traditional practices meet goal making data centers energy efficient environmentally sustainable world hardware operations team team data center responsible monitoring maintaining deploying physical infrastructure behind powerful search technology operations engineers deploy maintain google advanced data center server network infrastructure duties range physical deployment data related technology working closely various stakeholders team offers great introduction wider engineering world google google always engineering company hire people broad set technical skills ready address technology greatest challenges make impact millions billions users google engineers revolutionize search routinely work massive scalability storage solutions applications entirely new platforms developers around world google ads chrome android youtube social local google engineers changing world one technological achievement another responsibilities contribute repairs upgrades deployments cabling identify trends machine failure report bugs work remote teams resolve communicate effectively clients partners contribute projects deployment maintenance provide support current new data center infrastructure configure troubleshoot linux challenges servers resolve critical escalated technical challenges significant population affected equipment information collected processed part google careers profile job applications choose submit subject google applicant candidate privacy policy google proud equal opportunity affirmative action employer committed building workforce representative users serve creating culture belonging providing equal employment opportunity regardless race creed colour religion gender sexual orientation gender national origin disability age genetic information veteran status marital status pregnancy related condition including breastfeeding expecting criminal histories consistent legal requirements basis protected law see also google eeo policy know rights workplace discrimination illegal belonging google hire need requires accommodation please let us know completing accommodations applicants form google global company order facilitate efficient collaboration communication globally english proficiency requirement roles unless stated otherwise job posting recruitment agencies google accept agency cvs please forward cvs jobs alias google employees organisation location google responsible fees related unsolicited cvs follow life google us us contact us press related information investor relations blog equal opportunity google proud equal opportunity affirmative action employer committed building workforce representative users serve creating culture belonging providing equal employment opportunity regardless race creed colour religion gender sexual orientation gender national origin disability age genetic information veteran status marital status pregnancy related condition including breastfeeding expecting criminal histories consistent legal requirements basis protected law see also google eeo policy know rights workplace discrimination illegal belonging google hire privacy applicant candidate privacy terms help\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_name</th>\n",
       "      <th>resume_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data/Extracted/sample.pdf</td>\n",
       "      <td>madhava reddy creative programmer madhavso2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data/Extracted/sample2.pdf</td>\n",
       "      <td>experience education skills certifications vij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data/Extracted/sample3.pdf</td>\n",
       "      <td>experience education skills certifications vij...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  resume_name   \n",
       "0   Data/Extracted/sample.pdf  \\\n",
       "1  Data/Extracted/sample2.pdf   \n",
       "2  Data/Extracted/sample3.pdf   \n",
       "\n",
       "                                         resume_text  \n",
       "0  madhava reddy creative programmer madhavso2018...  \n",
       "1  experience education skills certifications vij...  \n",
       "2  experience education skills certifications vij...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def load_data(job_description_file, resumes_file):\n",
    "    resumes_df = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for resume in resumes_file:\n",
    "        text = \"\"\n",
    "        with fitz.open(resume) as pdf_document:\n",
    "            for page_number in range(pdf_document.page_count):\n",
    "                page = pdf_document[page_number]\n",
    "                text += page.get_text()\n",
    "        words = word_tokenize(text)\n",
    "        words = [word.lower() for word in words if word.isalnum()]\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        text = ' '.join(words)\n",
    "        resumes_df.append({'resume_name':resume, 'resume_text': text})\n",
    "    resumes_df = pd.DataFrame(resumes_df)\n",
    "\n",
    "    job_descriptions = \"\"\n",
    "    with fitz.open(job_description_file) as job_description_file:\n",
    "        for page_number in range(job_description_file.page_count):\n",
    "            page = job_description_file[page_number]\n",
    "            job_descriptions += page.get_text()\n",
    "    words = word_tokenize(job_descriptions)\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    job_descriptions = ' '.join(words)\n",
    "\n",
    "    return job_descriptions, resumes_df\n",
    "\n",
    "JD, resume_df = load_data('Data/sample_JD.pdf',['Data/Extracted/'+x for x in os.listdir('Data/Extracted/')])\n",
    "print(JD)\n",
    "resume_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analysis-of-job-description\"></a>\n",
    "## 5. Analysis of Job Description\n",
    "Here, we analyze the job description to identify key requirements and skills. This involves using TF-IDF (Term Frequency-Inverse Document Frequency) to extract important keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'google': 0.5045977978988233, 'data': 0.21625619909949573, 'related': 0.16819926596627444, 'center': 0.14417079939966382, 'engineering': 0.12014233283305317, 'equal': 0.12014233283305317, 'opportunity': 0.12014233283305317, 'status': 0.12014233283305317, 'technical': 0.12014233283305317, 'world': 0.12014233283305317, '2024': 0.09611386626644254, 'belonging': 0.09611386626644254, 'english': 0.09611386626644254, 'gender': 0.09611386626644254, 'information': 0.09611386626644254, 'infrastructure': 0.09611386626644254, 'please': 0.09611386626644254, 'resume': 0.09611386626644254, 'search': 0.09611386626644254, 'team': 0.09611386626644254, 'technology': 0.09611386626644254, 'us': 0.09611386626644254, 'users': 0.09611386626644254, 'ability': 0.07208539969983191, 'behind': 0.07208539969983191, 'challenges': 0.07208539969983191, 'current': 0.07208539969983191, 'cvs': 0.07208539969983191, 'degree': 0.07208539969983191, 'design': 0.07208539969983191, 'engineers': 0.07208539969983191, 'hire': 0.07208539969983191, 'job': 0.07208539969983191, 'know': 0.07208539969983191, 'linux': 0.07208539969983191, 'network': 0.07208539969983191, 'operations': 0.07208539969983191, 'physical': 0.07208539969983191, 'policy': 0.07208539969983191, 'privacy': 0.07208539969983191, 'requirements': 0.07208539969983191, 'teams': 0.07208539969983191, 'transcript': 0.07208539969983191, 'work': 0.07208539969983191, 'action': 0.04805693313322127, 'affirmative': 0.04805693313322127, 'age': 0.04805693313322127, 'also': 0.04805693313322127, 'applicant': 0.04805693313322127, 'application': 0.04805693313322127}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def analyze_job_role(job_description, stop_words):\n",
    "    stop_words.update([\"role\", \"responsibilities\", \"skills\", \"experience\", \"qualifications\"])  # Add additional custom stop words\n",
    "    job_description = ' '.join([word for word in job_description.lower().split() if word not in stop_words])\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([job_description])\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "    keyword_scores = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_keywords = dict([(keyword,score) for keyword, score in sorted_keywords[:50]])\n",
    "\n",
    "    return top_keywords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "job_keywords = analyze_job_role(JD, stop_words)\n",
    "print(job_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analysis-of-candidate-resume\"></a>\n",
    "## 6. Analysis of Candidate Resume\n",
    "Each candidate's resume is analyzed to extract relevant features and qualifications. We employ TF-IDF to identify important terms within the resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Extracted/sample.pdf\n",
      "{'data': 0.3283053930987497, 'learning': 0.2735878275822914, 'machine': 0.2735878275822914, '2022': 0.16415269654937484, 'cgpa': 0.16415269654937484, 'education': 0.16415269654937484, 'engineering': 0.16415269654937484, 'python': 0.16415269654937484, 'vijayawada': 0.16415269654937484, 'web': 0.16415269654937484, '2018': 0.10943513103291655, '2020': 0.10943513103291655, 'analysis': 0.10943513103291655, 'blackbucks': 0.10943513103291655, 'board': 0.10943513103291655, 'bot': 0.10943513103291655, 'cases': 0.10943513103291655, 'computer': 0.10943513103291655, 'development': 0.10943513103291655, 'engineers': 0.10943513103291655, 'india': 0.10943513103291655, 'ltd': 0.10943513103291655, 'project': 0.10943513103291655, 'pvt': 0.10943513103291655, 'science': 0.10943513103291655, 'technical': 0.10943513103291655, '2017': 0.054717565516458275, '2023': 0.054717565516458275, '2024': 0.054717565516458275, '9347156120': 0.054717565516458275, 'academic': 0.054717565516458275, 'aiding': 0.054717565516458275, 'aiml': 0.054717565516458275, 'andhra': 0.054717565516458275, 'ap': 0.054717565516458275, 'apply': 0.054717565516458275, 'beautifulsoup': 0.054717565516458275, 'certifications': 0.054717565516458275, 'certiﬁcation': 0.054717565516458275, 'certiﬁed': 0.054717565516458275, 'chaitanya': 0.054717565516458275, 'code': 0.054717565516458275, 'collected': 0.054717565516458275, 'complex': 0.054717565516458275, 'coordinated': 0.054717565516458275, 'coursera': 0.054717565516458275, 'created': 0.054717565516458275, 'creative': 0.054717565516458275, 'critical': 0.054717565516458275, 'css': 0.054717565516458275}\n",
      "\n",
      "\n",
      "\n",
      "Data/Extracted/sample2.pdf\n",
      "{'data': 0.21566554640687682, 'cgpa': 0.1617491598051576, 'development': 0.1617491598051576, 'engineering': 0.1617491598051576, 'project': 0.1617491598051576, 'system': 0.1617491598051576, 'university': 0.1617491598051576, '2018': 0.10783277320343841, '2020': 0.10783277320343841, 'analysis': 0.10783277320343841, 'bot': 0.10783277320343841, 'computer': 0.10783277320343841, 'disease': 0.10783277320343841, 'eapcet': 0.10783277320343841, 'fifa': 0.10783277320343841, 'high': 0.10783277320343841, 'intern': 0.10783277320343841, 'languages': 0.10783277320343841, 'leveraging': 0.10783277320343841, 'performance': 0.10783277320343841, 'prediction': 0.10783277320343841, 'presentations': 0.10783277320343841, 'programming': 0.10783277320343841, 'projects': 0.10783277320343841, 'python': 0.10783277320343841, 'reports': 0.10783277320343841, 'resulting': 0.10783277320343841, 'school': 0.10783277320343841, 'sql': 0.10783277320343841, 'students': 0.10783277320343841, '15': 0.053916386601719206, '2006': 0.053916386601719206, '2024': 0.053916386601719206, '40': 0.053916386601719206, '9666486245': 0.053916386601719206, 'academic': 0.053916386601719206, 'accommodations': 0.053916386601719206, 'achievements': 0.053916386601719206, 'adeptness': 0.053916386601719206, 'advanced': 0.053916386601719206, 'agile': 0.053916386601719206, 'aiml': 0.053916386601719206, 'algorithms': 0.053916386601719206, 'analytical': 0.053916386601719206, 'analytics': 0.053916386601719206, 'approaches': 0.053916386601719206, 'arrangements': 0.053916386601719206, 'assistance': 0.053916386601719206, 'average': 0.053916386601719206, 'beautifulsoup': 0.053916386601719206}\n",
      "\n",
      "\n",
      "\n",
      "Data/Extracted/sample3.pdf\n",
      "{'data': 0.21566554640687682, 'cgpa': 0.1617491598051576, 'development': 0.1617491598051576, 'engineering': 0.1617491598051576, 'project': 0.1617491598051576, 'system': 0.1617491598051576, 'university': 0.1617491598051576, '2018': 0.10783277320343841, '2020': 0.10783277320343841, 'analysis': 0.10783277320343841, 'bot': 0.10783277320343841, 'computer': 0.10783277320343841, 'disease': 0.10783277320343841, 'eapcet': 0.10783277320343841, 'fifa': 0.10783277320343841, 'high': 0.10783277320343841, 'intern': 0.10783277320343841, 'languages': 0.10783277320343841, 'leveraging': 0.10783277320343841, 'performance': 0.10783277320343841, 'prediction': 0.10783277320343841, 'presentations': 0.10783277320343841, 'programming': 0.10783277320343841, 'projects': 0.10783277320343841, 'python': 0.10783277320343841, 'reports': 0.10783277320343841, 'resulting': 0.10783277320343841, 'school': 0.10783277320343841, 'sql': 0.10783277320343841, 'students': 0.10783277320343841, '15': 0.053916386601719206, '2006': 0.053916386601719206, '2024': 0.053916386601719206, '40': 0.053916386601719206, '9666486245': 0.053916386601719206, 'academic': 0.053916386601719206, 'accommodations': 0.053916386601719206, 'achievements': 0.053916386601719206, 'adeptness': 0.053916386601719206, 'advanced': 0.053916386601719206, 'agile': 0.053916386601719206, 'aiml': 0.053916386601719206, 'algorithms': 0.053916386601719206, 'analytical': 0.053916386601719206, 'analytics': 0.053916386601719206, 'approaches': 0.053916386601719206, 'arrangements': 0.053916386601719206, 'assistance': 0.053916386601719206, 'average': 0.053916386601719206, 'beautifulsoup': 0.053916386601719206}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_candidate_resume(resume, stop_words):\n",
    "    stop_words.update([\"phone\", \"email\", \"address\", \"linkedin\", \"github\"])  # Add additional custom stop words\n",
    "    resume = ' '.join([word for word in resume.lower().split() if word not in stop_words])\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([resume])\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "    resume_features = dict(zip(feature_names, tfidf_scores))\n",
    "    \n",
    "    sorted_keywords = sorted(resume_features.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    resume_features = dict([(keyword,score) for keyword, score in sorted_keywords[:50]])\n",
    "\n",
    "    return resume_features\n",
    "\n",
    "for idx in range(len(resume_df)):\n",
    "    print(resume_df['resume_name'][idx])\n",
    "    candidate_features = analyze_candidate_resume(resume_df['resume_text'][idx],stop_words)\n",
    "    print(candidate_features, end=\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"applying-lda-model\"></a>\n",
    "## 7. Applying LDA Model (Demo)\n",
    "In this section, we demonstrate the application of the LDA (Latent Dirichlet Allocation) model on the job keywords. LDA helps uncover underlying topics within the job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus LDA:\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.5999131), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.5999131), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.5999131), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.10002683), (1, 0.10002423), (2, 0.10002682), (3, 0.5998953), (4, 0.10002682)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.5999131)]\n",
      "[(0, 0.5999131), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.10002309), (1, 0.100020856), (2, 0.10002308), (3, 0.1000199), (4, 0.5999131)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.5999131), (3, 0.10001989), (4, 0.100023076)]\n",
      "[(0, 0.10002308), (1, 0.100020856), (2, 0.5999131), (3, 0.100019895), (4, 0.10002308)]\n",
      "[(0, 0.5999131), (1, 0.100020856), (2, 0.10002308), (3, 0.1000199), (4, 0.10002308)]\n",
      "[(0, 0.5999131), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.5999131), (1, 0.100020856), (2, 0.10002308), (3, 0.100019895), (4, 0.10002308)]\n",
      "[(0, 0.10002558), (1, 0.5999012), (2, 0.10002558), (3, 0.10002205), (4, 0.10002558)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.5999131), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.10002682), (1, 0.10002424), (2, 0.10002683), (3, 0.5998953), (4, 0.10002682)]\n",
      "[(0, 0.100025564), (1, 0.5999012), (2, 0.100025564), (3, 0.10002204), (4, 0.10002557)]\n",
      "[(0, 0.10002683), (1, 0.10002424), (2, 0.10002683), (3, 0.59989524), (4, 0.10002683)]\n",
      "[(0, 0.10002557), (1, 0.5999012), (2, 0.10002558), (3, 0.10002205), (4, 0.10002558)]\n",
      "[(0, 0.5999131), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.10002684), (1, 0.100024246), (2, 0.10002684), (3, 0.59989524), (4, 0.10002683)]\n",
      "[(0, 0.10002682), (1, 0.10002424), (2, 0.10002682), (3, 0.5998953), (4, 0.10002683)]\n",
      "[(0, 0.5999131), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.10002308), (1, 0.10002086), (2, 0.10002308), (3, 0.1000199), (4, 0.5999131)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.5999131), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.5999131), (1, 0.10002085), (2, 0.100023076), (3, 0.10001989), (4, 0.100023076)]\n",
      "[(0, 0.10002557), (1, 0.59990126), (2, 0.10002557), (3, 0.10002205), (4, 0.10002557)]\n",
      "[(0, 0.10002684), (1, 0.10002425), (2, 0.100026846), (3, 0.5998952), (4, 0.100026846)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.100023076), (3, 0.10001989), (4, 0.5999131)]\n",
      "[(0, 0.10002684), (1, 0.100024246), (2, 0.10002684), (3, 0.59989524), (4, 0.10002683)]\n",
      "[(0, 0.10002307), (1, 0.10002084), (2, 0.10002307), (3, 0.10001989), (4, 0.5999131)]\n",
      "[(0, 0.10002558), (1, 0.5999012), (2, 0.10002558), (3, 0.10002205), (4, 0.10002558)]\n",
      "[(0, 0.10002683), (1, 0.100024246), (2, 0.10002683), (3, 0.59989524), (4, 0.10002683)]\n",
      "[(0, 0.10002559), (1, 0.59990126), (2, 0.10002559), (3, 0.10002205), (4, 0.10002558)]\n",
      "[(0, 0.100023076), (1, 0.100020856), (2, 0.5999131), (3, 0.100019895), (4, 0.100023076)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.100023076), (3, 0.10001989), (4, 0.5999131)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.5999131), (3, 0.100019895), (4, 0.10002308)]\n",
      "[(0, 0.10002308), (1, 0.100020856), (2, 0.5999131), (3, 0.1000199), (4, 0.10002308)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.5999131)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.5999131)]\n",
      "[(0, 0.10002558), (1, 0.5999012), (2, 0.10002557), (3, 0.10002205), (4, 0.10002558)]\n",
      "[(0, 0.10002683), (1, 0.10002424), (2, 0.10002683), (3, 0.59989524), (4, 0.10002683)]\n",
      "[(0, 0.10002559), (1, 0.5999012), (2, 0.100025594), (3, 0.100022055), (4, 0.10002559)]\n",
      "[(0, 0.100025564), (1, 0.5999012), (2, 0.10002557), (3, 0.10002204), (4, 0.100025564)]\n",
      "[(0, 0.10002683), (1, 0.10002424), (2, 0.10002683), (3, 0.59989524), (4, 0.10002683)]\n",
      "[(0, 0.10002557), (1, 0.5999012), (2, 0.100025564), (3, 0.10002204), (4, 0.10002557)]\n",
      "[(0, 0.10002559), (1, 0.59990126), (2, 0.10002558), (3, 0.10002205), (4, 0.10002558)]\n",
      "[(0, 0.10002683), (1, 0.10002424), (2, 0.10002683), (3, 0.59989524), (4, 0.10002683)]\n",
      "[(0, 0.100023076), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.5999131)]\n",
      "[(0, 0.10002684), (1, 0.100024246), (2, 0.10002683), (3, 0.59989524), (4, 0.10002684)]\n",
      "[(0, 0.5999131), (1, 0.10002085), (2, 0.100023076), (3, 0.100019895), (4, 0.100023076)]\n",
      "\n",
      "Dictionary:\n",
      "google: 0\n",
      "data: 1\n",
      "related: 2\n",
      "center: 3\n",
      "engineering: 4\n",
      "equal: 5\n",
      "opportunity: 6\n",
      "status: 7\n",
      "technical: 8\n",
      "world: 9\n",
      "2024: 10\n",
      "belonging: 11\n",
      "english: 12\n",
      "gender: 13\n",
      "information: 14\n",
      "infrastructure: 15\n",
      "please: 16\n",
      "resume: 17\n",
      "search: 18\n",
      "team: 19\n",
      "technology: 20\n",
      "us: 21\n",
      "users: 22\n",
      "ability: 23\n",
      "behind: 24\n",
      "challenges: 25\n",
      "current: 26\n",
      "cvs: 27\n",
      "degree: 28\n",
      "design: 29\n",
      "engineers: 30\n",
      "hire: 31\n",
      "job: 32\n",
      "know: 33\n",
      "linux: 34\n",
      "network: 35\n",
      "operations: 36\n",
      "physical: 37\n",
      "policy: 38\n",
      "privacy: 39\n",
      "requirements: 40\n",
      "teams: 41\n",
      "transcript: 42\n",
      "work: 43\n",
      "action: 44\n",
      "affirmative: 45\n",
      "age: 46\n",
      "also: 47\n",
      "applicant: 48\n",
      "application: 49\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "def apply_lda(texts):\n",
    "    tokenized_texts = [text.lower().split() for text in texts]\n",
    "    dictionary = corpora.Dictionary(tokenized_texts)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "    num_topics = 5  # Adjust the number of topics based on your requirements\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    \n",
    "    corpus_lda = lda_model[corpus]\n",
    "    \n",
    "    return lda_model, corpus_lda, dictionary\n",
    "\n",
    "lda_model, corpus_lda, dictionary = apply_lda(job_keywords)\n",
    "\n",
    "print(\"Corpus LDA:\")\n",
    "for doc in corpus_lda:\n",
    "    print(doc)\n",
    "    \n",
    "print(\"\\nDictionary:\")\n",
    "for word, index in dictionary.token2id.items():\n",
    "    print(f\"{word}: {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"calculating-similarity\"></a>\n",
    "## 8. Calculating Similarity between JD and Resumes\n",
    "We calculate the similarity between the job description and each resume. This involves computing cosine similarity between TF-IDF vectors and LDA topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7175028329875784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(job_keywords, resume_features, lda_model, corpus_lda, dictionary):\n",
    "     # Compute cosine similarity between TF-IDF vectors of job keywords and resume features\n",
    "    tfidf_cosine_similarity = cosine_similarity([list(job_keywords.values())], [list(resume_features.values())])[0][0]\n",
    "\n",
    "    # Convert resume text to LDA vector\n",
    "    resume_lda_vector = lda_model[dictionary.doc2bow(resume_features.keys())]\n",
    "\n",
    "    # Compute similarity between job LDA topics and resume LDA vector\n",
    "    lda_similarity = 0\n",
    "    for topic_id, topic_score in resume_lda_vector:\n",
    "        lda_similarity += topic_score * corpus_lda[0][topic_id][1]  # Weighted similarity based on LDA scores\n",
    "\n",
    "    # Combine both similarities (you can adjust the weights based on importance)\n",
    "    final_score = 0.7 * tfidf_cosine_similarity + 0.3 * lda_similarity\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "print(calculate_similarity(job_keywords, analyze_candidate_resume(resume_df['resume_text'][1],stop_words), lda_model, corpus_lda, dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ranking-candidates\"></a>\n",
    "## 9. Ranking Candidates\n",
    "Candidates are ranked based on their similarity to the job description. The ranking helps identify the most suitable candidates for the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sample.pdf', 0.7250509821243742), ('sample2.pdf', 0.7025081332002299), ('sample3.pdf', 0.7025081332002299)]\n"
     ]
    }
   ],
   "source": [
    "def rank_candidates(job_description, resumes, stop_words):\n",
    "    job_keywords = analyze_job_role(job_description, stop_words)\n",
    "    lda_model, corpus_lda, dictionary = apply_lda(job_description)\n",
    "\n",
    "    ranking_scores = []\n",
    "    for resume in resumes[\"resume_text\"][:]:\n",
    "        resume_features = analyze_candidate_resume(resume, stop_words)\n",
    "        score = calculate_similarity(job_keywords, resume_features, lda_model, corpus_lda, dictionary)\n",
    "        ranking_scores.append(score)\n",
    "    ranked_candidates = sorted(enumerate(ranking_scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_candidates\n",
    "\n",
    "ranked_candidates = rank_candidates(JD, resume_df, stop_words)\n",
    "leaderboard_data = [(resume_df.iloc[idx]['resume_name'].split(\"/\")[-1], score) for idx, score in ranked_candidates]\n",
    "print(leaderboard_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clearing-extracted-files\"></a>\n",
    "## 10. Clearing Extracted Files\n",
    "Finally, we clean up by removing the extracted files to maintain a tidy workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file: Data\\Extracted\\sample.pdf\n",
      "Deleted file: Data\\Extracted\\sample2.pdf\n",
      "Deleted file: Data\\Extracted\\sample3.pdf\n"
     ]
    }
   ],
   "source": [
    "def clear_directory(directory):\n",
    "    # Iterate over all the files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            # Check if it is a file\n",
    "            if os.path.isfile(file_path):\n",
    "                # Delete the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "            \n",
    "clear_directory('Data\\Extracted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
